{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bihari Tokenizer.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOiNOx22AYk4ljHnAdyyUTu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SunitKolabkar/NLP_Codes/blob/master/Bihari_Tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSJpodbvPE5e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 705
        },
        "outputId": "8adad27e-4c50-4ca1-ca45-b0ee670077bd"
      },
      "source": [
        "# Install `transformers` from master\n",
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip list | grep -E 'transformers|tokenizers'\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-5b1jnbm9\n",
            "  Running command git clone -q https://github.com/huggingface/transformers /tmp/pip-req-build-5b1jnbm9\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==2.10.0) (1.18.4)\n",
            "Collecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==2.10.0) (20.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.10.0) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.10.0) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==2.10.0) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.10.0) (2019.12.20)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 31.2MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 50.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from transformers==2.10.0) (0.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==2.10.0) (1.12.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==2.10.0) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.10.0) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.10.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.10.0) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.10.0) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.10.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.10.0) (0.15.1)\n",
            "Building wheels for collected packages: transformers, sacremoses\n",
            "  Building wheel for transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-2.10.0-cp36-none-any.whl size=665110 sha256=9b6b86a566309d74d19b6ac6e7ac101313464467037762ac65bdd3c3730d493c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-apvrpbi5/wheels/70/d3/52/b3fa4f8b8ef04167ac62e5bb2accb62ae764db2a378247490e\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=866a812f31a7ba5022137d507c0175e18ee658dc87ffabf4f52cd275cc802cfa\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built transformers sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.7.0 transformers-2.10.0\n",
            "tokenizers               0.7.0          \n",
            "transformers             2.10.0         \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6mjJBCgQF0v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pathlib import Path\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "\n",
        "paths = [str(x) for x in Path(\".\").glob(\"**/*.txt\")]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCa5rXgUQSte",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#Initialize tokenizer\n",
        "tokenizer = ByteLevelBPETokenizer()\n",
        "\n",
        "#Customize training\n",
        "tokenizer.train(files=paths, vocab_size=10000,\n",
        "                min_frequency=2, special_tokens=[\n",
        "                                                 \"<s>\",\n",
        "                                                 \"<pad>\",\n",
        "                                                 \"</s>\",\n",
        "                                                 \"<unk>\",\n",
        "                                                 \"<mask>\",\n",
        "                ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeIavV9QQYr5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cd6bc68d-eaf4-4456-a05e-4f7f94d1b236"
      },
      "source": [
        "!mkdir BihariBERT\n",
        "tokenizer.save(\"BihariBERT\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['BihariBERT/vocab.json', 'BihariBERT/merges.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUDXA2p7RTrl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tokenizers.implementations import ByteLevelBPETokenizer\n",
        "from tokenizers.processors import BertProcessing\n",
        "\n",
        "tokenizer = ByteLevelBPETokenizer(\n",
        "    './BihariBERT/vocab.json',\n",
        "    'BihariBERT/merges.txt',\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCizMZ7hRr-M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer._tokenizer.post_processor = BertProcessing(\n",
        "    ('</s>', tokenizer.token_to_id('</s>')),\n",
        "    ('<s>', tokenizer.token_to_id('<s>')),\n",
        ")\n",
        "tokenizer.enable_truncation(max_length=512)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNnaY-IASIpO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 943
        },
        "outputId": "31576a83-db63-4a6e-a252-6f44901cf96c"
      },
      "source": [
        "#tokenizer.encode('ई सेहत आ स्वास्थ्य-संबंधी लेख').tokens\n",
        "tokenizer.encode('नीतीश सरकार व RJD में आरपार, तेजस्‍वी-राबड़ी कभी भी हो सकते गिरफ्तार').tokens"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<s>',\n",
              " 'à¤¨',\n",
              " 'à¥Ģ',\n",
              " 'à¤¤',\n",
              " 'à¥Ģ',\n",
              " 'à¤¶',\n",
              " 'Ġà¤¸',\n",
              " 'à¤°à¤ķ',\n",
              " 'à¤¾',\n",
              " 'à¤°',\n",
              " 'Ġà¤µ',\n",
              " 'Ġ',\n",
              " 'R',\n",
              " 'J',\n",
              " 'D',\n",
              " 'Ġà¤®',\n",
              " 'à¥ĩà¤Ĥ',\n",
              " 'Ġà¤Ĩ',\n",
              " 'à¤°',\n",
              " 'à¤ª',\n",
              " 'à¤¾',\n",
              " 'à¤°',\n",
              " ',',\n",
              " 'Ġà¤¤',\n",
              " 'à¥ĩ',\n",
              " 'à¤ľà¤¸',\n",
              " 'à¥į',\n",
              " 'âĢ',\n",
              " 'į',\n",
              " 'à¤µ',\n",
              " 'à¥Ģ',\n",
              " '-',\n",
              " 'à¤°',\n",
              " 'à¤¾',\n",
              " 'à¤¬',\n",
              " 'à¤¡',\n",
              " 'à¤¼à¥Ģ',\n",
              " 'Ġà¤ķ',\n",
              " 'à¤Ń',\n",
              " 'à¥Ģ',\n",
              " 'Ġà¤Ń',\n",
              " 'à¥Ģ',\n",
              " 'Ġà¤¹',\n",
              " 'à¥ĭ',\n",
              " 'Ġà¤¸à¤ķà¤¤',\n",
              " 'à¥ĩ',\n",
              " 'Ġà¤Ĺ',\n",
              " 'à¤¿',\n",
              " 'à¤°à¤«',\n",
              " 'à¥į',\n",
              " 'à¤¤',\n",
              " 'à¤¾',\n",
              " 'à¤°',\n",
              " '</s>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ohgc1EalSYzG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}